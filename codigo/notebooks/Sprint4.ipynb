{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7daac5f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyxlsb in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.0.10)\n",
      "Requirement already satisfied: pandas_market_calendars in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (4.4.0)\n",
      "Requirement already satisfied: pandas>=1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas_market_calendars) (1.5.3)\n",
      "Requirement already satisfied: pytz in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas_market_calendars) (2024.1)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas_market_calendars) (2.8.2)\n",
      "Requirement already satisfied: exchange-calendars>=3.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas_market_calendars) (4.5.3)\n",
      "Requirement already satisfied: numpy<2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.26.4)\n",
      "Requirement already satisfied: pyluach in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.2.0)\n",
      "Requirement already satisfied: toolz in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.1)\n",
      "Requirement already satisfied: tzdata in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2024.1)\n",
      "Requirement already satisfied: korean-lunar-calendar in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil->pandas_market_calendars) (1.16.0)\n",
      "Requirement already satisfied: stable-baselines3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from stable-baselines3) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from stable-baselines3) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from stable-baselines3) (2.2.2)\n",
      "Requirement already satisfied: cloudpickle in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from stable-baselines3) (2.2.1)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from stable-baselines3) (1.5.3)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from stable-baselines3) (3.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.13.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3) (12.4.99)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (21.3)\n",
      "Requirement already satisfied: pillow>=8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyxlsb\n",
    "!pip install pandas_market_calendars\n",
    "!pip install stable-baselines3\n",
    "!pip install shimmy>=0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c5a8e9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/fsspec/registry.py:273: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cód. Cliente  Dt. Operação Tipo Operação Cód. Título Cód. Corretora  \\\n",
      "0    Cliente 1         45057             C       GGBR4           FLOW   \n",
      "1    Cliente 2         45057             C      SANB11           GARA   \n",
      "2    Cliente 3         45057             C       PRIO3           CITI   \n",
      "3    Cliente 4         45225             C       WEGE3           XPIN   \n",
      "4    Cliente 5         44964             C       VALE3           INDU   \n",
      "\n",
      "   Quantidade      Preço  Valor Líquido  Dt. Liquidação Tipo  \n",
      "0         112  24.120000       -2704.11           45061   AV  \n",
      "1           6  30.060000        -180.58           45061   AV  \n",
      "2         900  34.540000      -31115.09           45061   AV  \n",
      "3         100  31.800600       -3180.06           45229   AV  \n",
      "4         900  87.354911      -78619.42           44966   AV  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the S3 path to your .xlsx file\n",
    "file_path = '../data'\n",
    "\n",
    "# Read the Excel file directly into a pandas DataFrame\n",
    "df_purchase = pd.read_excel(file_path + \"base2023_compra.xlsb\", engine=\"pyxlsb\")\n",
    "df_purchase_2 = pd.read_excel(file_path + \"base2023_compra_2.xlsb\", engine=\"pyxlsb\")\n",
    "\n",
    "# Now you can work with the DataFrame 'data'\n",
    "print(df_purchase.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d7b2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the date columns to pandas datetime\n",
    "df_purchase_datetime = df_purchase.copy(deep=True)\n",
    "df_purchase_2_datetime = df_purchase_2.copy(deep=True)\n",
    "\n",
    "df_purchase_datetime['Dt. Operação'] = pd.to_datetime(df_purchase_datetime['Dt. Operação'], unit='D', origin='1899-12-30')\n",
    "df_purchase_2_datetime['Dt. Operação'] = pd.to_datetime(df_purchase_2_datetime['Dt. Operação'], format=\"%d/%m/%Y\")\n",
    "\n",
    "df_purchase_2_datetime.rename(columns={\"Vencimento\": \"Dt. Liquidação\"}, inplace=True)\n",
    "\n",
    "df_purchase_datetime['Dt. Liquidação'] = pd.to_datetime(df_purchase_datetime['Dt. Liquidação'], unit='D', origin='1899-12-30')\n",
    "df_purchase_2_datetime['Dt. Liquidação'] = pd.to_datetime(df_purchase_2_datetime['Dt. Liquidação'], errors='coerce', format=\"%d/%m/%Y\")\n",
    "\n",
    "df_purchases = pd.concat([df_purchase_datetime, df_purchase_2_datetime], ignore_index=True)\n",
    "df_purchases = df_purchases.sort_values(by=[\"Cód. Cliente\", \"Dt. Operação\", \"Cód. Título\", \"Cód. Corretora\"]).reset_index()\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3e2d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = pd.read_excel(file_path + \"base2023_venda.xlsb\", engine=\"pyxlsb\")\n",
    "\n",
    "df_sales_datetime = df_sales.copy(deep=True)\n",
    "df_sales_datetime['Dt. Operação'] = pd.to_datetime(df_sales_datetime['Dt. Operação'], unit='D', origin='1899-12-30')\n",
    "df_sales_datetime['Vencimento'] = pd.to_datetime(df_sales_datetime['Vencimento'], unit='D', origin='1899-12-30')\n",
    "df_sales_datetime = df_sales_datetime.sort_values(by=[\"Cód. Cliente\", \"Dt. Operação\", \"Cód. Título\", \"Cód. Corretora\"]).reset_index()\n",
    "\n",
    "df_filtered_purchases = df_purchases.copy(deep=True)\n",
    "df_filtered_sales = df_sales_datetime.copy(deep=True)\n",
    "\n",
    "df_filtered_purchases = df_filtered_purchases.drop([\"index\", \"Tipo Operação\", \"Valor Líquido\", \"Dt. Liquidação\", \"Tipo\"], axis=1)\n",
    "df_filtered_sales = df_filtered_sales.drop([\"index\", \"Tipo Operação\", \"Valor Líquido\", \"Tipo\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c66c95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chave</th>\n",
       "      <th>Cód. Cliente</th>\n",
       "      <th>Dt. Operação</th>\n",
       "      <th>Cód. Título</th>\n",
       "      <th>Cód. Corretora</th>\n",
       "      <th>Quantidade</th>\n",
       "      <th>Preço</th>\n",
       "      <th>Vencimento</th>\n",
       "      <th>DI</th>\n",
       "      <th>du</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4721A</td>\n",
       "      <td>Cliente 1</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>XPBR31</td>\n",
       "      <td>XPIN</td>\n",
       "      <td>4470</td>\n",
       "      <td>76.450544</td>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>0.136781</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19007A</td>\n",
       "      <td>Cliente 1</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>XPBR31</td>\n",
       "      <td>XPIN</td>\n",
       "      <td>35530</td>\n",
       "      <td>76.440559</td>\n",
       "      <td>2023-03-21</td>\n",
       "      <td>0.136781</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20955A</td>\n",
       "      <td>Cliente 1</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>XPBR31</td>\n",
       "      <td>XPIN</td>\n",
       "      <td>5440</td>\n",
       "      <td>76.069360</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>0.136704</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30031A</td>\n",
       "      <td>Cliente 1</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>XPBR31</td>\n",
       "      <td>XPIN</td>\n",
       "      <td>14560</td>\n",
       "      <td>76.079336</td>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>0.136704</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18899A</td>\n",
       "      <td>Cliente 1</td>\n",
       "      <td>2023-01-19</td>\n",
       "      <td>XPBR31</td>\n",
       "      <td>XPIN</td>\n",
       "      <td>41230</td>\n",
       "      <td>86.807599</td>\n",
       "      <td>2023-02-22</td>\n",
       "      <td>0.136585</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Chave Cód. Cliente Dt. Operação Cód. Título Cód. Corretora  Quantidade  \\\n",
       "0   4721A    Cliente 1   2023-01-05      XPBR31           XPIN        4470   \n",
       "1  19007A    Cliente 1   2023-01-05      XPBR31           XPIN       35530   \n",
       "2  20955A    Cliente 1   2023-01-05      XPBR31           XPIN        5440   \n",
       "3  30031A    Cliente 1   2023-01-05      XPBR31           XPIN       14560   \n",
       "4  18899A    Cliente 1   2023-01-19      XPBR31           XPIN       41230   \n",
       "\n",
       "       Preço Vencimento        DI  du  \n",
       "0  76.450544 2023-03-21  0.136781  51  \n",
       "1  76.440559 2023-03-21  0.136781  51  \n",
       "2  76.069360 2023-03-06  0.136704  40  \n",
       "3  76.079336 2023-03-06  0.136704  40  \n",
       "4  86.807599 2023-02-22  0.136585  22  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas_market_calendars as mcal\n",
    "brazil_calendar = mcal.get_calendar('BMF')\n",
    "\n",
    "def calculate_du(x):\n",
    "  business_days = brazil_calendar.valid_days(x[\"Dt. Operação\"], x[\"Vencimento\"])\n",
    "  du = len(business_days) - 1\n",
    "  return du\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "df_filtered_sales[\"du\"] = df_filtered_sales.apply(calculate_du, axis=1)\n",
    "\n",
    "df_filtered_sales_du = df_filtered_sales.copy(deep=True)\n",
    "df_filtered_sales_du = df_filtered_sales[(df_filtered_sales[\"du\"] > 0)].sort_values(by=['Cód. Cliente', 'Dt. Operação', 'Cód. Título', 'Cód. Corretora'])\n",
    "display(df_filtered_sales_du.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf1858d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:32,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "filtered_dfs = []\n",
    "\n",
    "for index, row in tqdm(df_filtered_sales_du.sample(100).iterrows()):\n",
    "  available_purchases = df_filtered_purchases[\n",
    "    (df_filtered_purchases['Cód. Cliente'] == row['Cód. Cliente']) &\n",
    "    (df_filtered_purchases['Dt. Operação'] == row['Dt. Operação']) &\n",
    "    (df_filtered_purchases['Cód. Corretora'] == row['Cód. Corretora']) &\n",
    "    (df_filtered_purchases['Cód. Título'] == row['Cód. Título'])\n",
    "  ]\n",
    "\n",
    "  if not available_purchases.empty:\n",
    "    filtered_dfs.append({\n",
    "        'sale': row.copy(deep=True),\n",
    "        'purchase': available_purchases.copy(deep=True)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b4918",
   "metadata": {},
   "source": [
    "# Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7d7fbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2815206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_current_price(purchases_quantities, purchases): # ((preco, quantidade), (preco, quantidade), ...)\n",
    "    total_price = 0\n",
    "    total_quantity = 0\n",
    "\n",
    "    for purchase in range(len(purchases_quantities)):\n",
    "        total_price += purchases.iloc[purchase][\"Preço\"] * purchases_quantities[purchase]\n",
    "        total_quantity += purchases_quantities[purchase]\n",
    "\n",
    "    if total_quantity == 0:\n",
    "      return 0\n",
    "\n",
    "    return total_price / total_quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a92abc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ideal_price(sale_price, du, di):\n",
    "  #pi=pv/(di+1)^du/252\n",
    "  ideal_price = sale_price / ((di + 1) ** (du / 252))\n",
    "  return ideal_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fb84f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cdi(du, purchase_price, sale_price, di):\n",
    "    if purchase_price == 0:\n",
    "      return 0\n",
    "    rent = ((sale_price / purchase_price) - 1)\n",
    "    annual_rent = ((1 + rent) ** (252 / du)) - 1\n",
    "\n",
    "    cdi = (annual_rent / di) * 100\n",
    "\n",
    "    return cdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d776cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def step_reward(sale_price, ideal_price, current_price, old_price, is_valid):\n",
    "\n",
    "    if not is_valid:\n",
    "        return -1e3\n",
    "\n",
    "    reward = 0\n",
    "\n",
    "    if (ideal_price - current_price) > (ideal_price - old_price):\n",
    "      reward = -100\n",
    "    elif (ideal_price - current_price) < (ideal_price - old_price):\n",
    "      reward = 100\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def episode_reward(di, du, current_price, sale_price):\n",
    "    cdi = calculate_cdi(du, current_price, sale_price, di)\n",
    "\n",
    "    if (99.0 <= cdi <= 101.0):\n",
    "        return 1e4\n",
    "    elif (101.0 < cdi <= 110.0):\n",
    "        return 1e2\n",
    "    elif (80.0 <= cdi <= 98.9):\n",
    "        return 1e-1\n",
    "    else:\n",
    "        return 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d3c9a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataframes(current_sale_index, dataframes, purchases_quantities):\n",
    "  # Critérios do 'current_episode'\n",
    "  current_sale = dataframes[current_sale_index]['sale']\n",
    "\n",
    "  # Loop através dos 'episodes' começando do próximo após 'current_episode'\n",
    "  for sale_index in range(current_sale_index + 1, len(dataframes)):\n",
    "      sale = filtered_dfs[sale_index]['sale']\n",
    "\n",
    "      # Verifica se os critérios são atendidos\n",
    "      criterios = (sale['Cód. Cliente'] == current_sale['Cód. Cliente'] and\n",
    "                  sale['Dt. Operação'] == current_sale['Dt. Operação'] and\n",
    "                  sale['Cód. Título'] == current_sale['Cód. Título'] and\n",
    "                  sale['Cód. Corretora'] == current_sale['Cód. Corretora'])\n",
    "\n",
    "      if not criterios:\n",
    "          break\n",
    "\n",
    "      purchases = dataframes[sale_index]['purchase']\n",
    "      # Atualize as quantidades baseado no dicionário index_quantidade\n",
    "      for i in range(len(purchases_quantities)):\n",
    "        purchases.iloc[i, purchases.columns.get_loc(\"Quantidade\")] -= purchases_quantities[i]\n",
    "\n",
    "  return dataframes\n",
    "  # Note que o loop 'for' verifica todos os episódios, mesmo que não cumpram os critérios,\n",
    "  # mas apenas executa a atualização de quantidade se os critérios forem atendidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b6f27fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_data = [\n",
    "  \"sale_price\",\n",
    "  \"sale_quantity\",\n",
    "  \"combination_current_price\",\n",
    "  \"purchase_price\",\n",
    "  \"purchase_quantity\",\n",
    "  \"ideal_price\",\n",
    "  \"purchase_combined_quantity\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0400495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "class Environment(Env):\n",
    "    def __init__(self):\n",
    "        self.filtered_dfs = deepcopy(filtered_dfs)\n",
    "\n",
    "        #self.actions = [1, 0.75, 0.50, 0.25, 0, -0.25, -0.50, -0.75, -1]\n",
    "        self.actions = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0, -0.1, -0.2, -0.3, -0.4, -0.5, -0.6, -0.7, -0.8, -0.9, -1]\n",
    "        self.observation_space = Box(low=-np.inf, high=np.inf, shape=(len(observation_data),), dtype=np.float64)\n",
    "\n",
    "        self.quantities_price = [(0,0)]\n",
    "        #self.quantities_index = np.zeros(1)\n",
    "\n",
    "        self.all_quantities = [np.zeros(x['purchase'].shape[0]) for x in filtered_dfs]\n",
    "\n",
    "        self.cdis = np.zeros(len(self.filtered_dfs))\n",
    "        self.best_cdis = np.zeros(len(self.filtered_dfs))\n",
    "\n",
    "        # Quantas ações podem ser tomadas\n",
    "        self.action_space = Discrete(len(self.actions))\n",
    "\n",
    "        self.current_episode = 0\n",
    "        self.current_step = 0\n",
    "\n",
    "        self.max_steps = 0\n",
    "\n",
    "        self.du = 0\n",
    "        self.di = 0\n",
    "\n",
    "        self.state = None\n",
    "\n",
    "    # atualizar as quantidades de compra e venda de acordo com a ação tomada no estado S\n",
    "    def update_data(self, action):\n",
    "      is_valid = True\n",
    "      current_purchase_index = self.current_step % self.purchases.shape[0]\n",
    "\n",
    "\n",
    "      if action < 0:\n",
    "        if self.all_quantities[self.current_episode][current_purchase_index] == 0:\n",
    "            is_valid = False\n",
    "\n",
    "        quantity_of_purchase = int(action * self.all_quantities[self.current_episode][current_purchase_index])\n",
    "      else:\n",
    "        quantity_of_purchase = int(action * self.purchases.iloc[current_purchase_index]['Quantidade'])\n",
    "\n",
    "      if quantity_of_purchase < 0:\n",
    "        if abs(quantity_of_purchase) > self.all_quantities[self.current_episode][current_purchase_index]:\n",
    "          quantity_of_purchase = -1 * self.all_quantities[self.current_episode][current_purchase_index]\n",
    "\n",
    "      # se ele estiver tentando pegar mais do que tem na quantidade de venda\n",
    "      # ele pega o que tem na quantidade de venda\n",
    "      if quantity_of_purchase - self.sale[\"Quantidade\"] > 0:\n",
    "        quantity_of_purchase = self.sale[\"Quantidade\"]\n",
    "\n",
    "      self.all_quantities[self.current_episode][current_purchase_index] += quantity_of_purchase\n",
    "\n",
    "      self.sale[\"Quantidade\"] = self.sale[\"Quantidade\"] - quantity_of_purchase\n",
    "      # print(\"action \", action)\n",
    "      #print(\"compras\", self.purchases.iloc[current_purchase_index][\"Quantidade\"])\n",
    "      self.purchases.iloc[current_purchase_index, self.purchases.columns.get_loc(\"Quantidade\")] -= quantity_of_purchase\n",
    "      # print(\"acao: \", action)\n",
    "      # print(\"quantity_of_purchase: \", quantity_of_purchase)\n",
    "      # print(\"current purchase \", current_purchase_index)\n",
    "      # print(\"Sales: \", self.sale[\"Quantidade\"])\n",
    "\n",
    "      return quantity_of_purchase, is_valid\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        quantidade_compra = self.actions[action]\n",
    "\n",
    "        quantidade_ajustada, is_valid = self.update_data(quantidade_compra)\n",
    "\n",
    "        preco_medio_antigo = self.state[2]\n",
    "        preco_venda = self.state[0]\n",
    "\n",
    "        self.quantities_price.append((self.state[3], quantidade_ajustada))\n",
    "\n",
    "        preco_medio_at = calculate_current_price(self.all_quantities[self.current_episode], self.purchases)\n",
    "\n",
    "        # calcular recompensa\n",
    "        # reward = calculate_reward(self.du, self.di, preco_medio_at, preco_venda, is_valid)\n",
    "        reward = step_reward(self.state[0], self.state[5], preco_medio_at, preco_medio_antigo, is_valid)\n",
    "\n",
    "        # atualizar variaveis\n",
    "        self.current_step += 1\n",
    "\n",
    "        # atualizar estado\n",
    "        self.state = self.__get_observation__()\n",
    "\n",
    "        # condicao de parada\n",
    "        cdi = calculate_cdi(self.du, preco_medio_at, self.state[0], self.di)\n",
    "        if (self.state[1] == 0\n",
    "             and 100 <= cdi <= 104):\n",
    "          reward += 1000\n",
    "          done = True\n",
    "          self.cdis[self.current_episode] = cdi\n",
    "          self.best_cdis[self.current_episode] = cdi\n",
    "\n",
    "        elif self.current_step >= self.max_steps:\n",
    "          original_sale_quantity = filtered_dfs[self.current_episode]['sale']['Quantidade']\n",
    "          sale_completion = sum(self.all_quantities[self.current_episode]) / original_sale_quantity\n",
    "\n",
    "          reward += -100 + 10**sale_completion\n",
    "          if sum(self.all_quantities[self.current_episode]) == 0:\n",
    "            reward = -1e3\n",
    "          done = True\n",
    "          self.cdis[self.current_episode] = cdi\n",
    "\n",
    "        #print(\"reward: \", reward)\n",
    "\n",
    "        if done:\n",
    "          #print(\"quantities_index: \", self.all_quantities[self.current_episode])\n",
    "          #print(self.state)\n",
    "          #print(\"episodio: \", self.current_episode)\n",
    "          #print(\"final reward: \", reward)\n",
    "          self.filtered_dfs = update_dataframes(self.current_episode, self.filtered_dfs, self.all_quantities[self.current_episode])\n",
    "          self.current_episode += 1\n",
    "          reward += episode_reward(self.du, self.di, preco_medio_at, preco_venda)\n",
    "\n",
    "        info = {}\n",
    "        return self.state.reshape(len(observation_data),), reward, done, info\n",
    "\n",
    "    def render(self, mode):\n",
    "        # Pode ser implementado para \"Exibir\" o cenário\n",
    "        # tipo o gráfico do grid do gridworld\n",
    "        # opcional\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_episode = self.current_episode % len(self.filtered_dfs)\n",
    "        self.purchases = self.filtered_dfs[self.current_episode]['purchase']\n",
    "        self.sale = self.filtered_dfs[self.current_episode]['sale']\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.quantities_price = [(0, 0)]\n",
    "        self.state = self.__get_observation__()\n",
    "        #self.quantities_index = np.zeros(self.purchases.shape[0])\n",
    "        return self.state.reshape(len(observation_data), )\n",
    "\n",
    "    def __get_observation__(self):\n",
    "        quantidade_linhas_compra = self.purchases.shape[0]\n",
    "        index_compra_atual = self.current_step % quantidade_linhas_compra\n",
    "\n",
    "        self.di = self.sale['DI']\n",
    "        preco_venda = self.sale['Preço']\n",
    "        self.du = self.sale['du']\n",
    "\n",
    "        preco_ideal = calculate_ideal_price(preco_venda, self.du, self.di) #implentar\n",
    "\n",
    "        quantidade_venda = self.sale['Quantidade']\n",
    "\n",
    "        if quantidade_venda > 100:\n",
    "          quantidade_venda /= 100\n",
    "          if quantidade_venda > 100:\n",
    "            quantidade_venda /= 10\n",
    "\n",
    "        preco_compra = self.purchases.iloc[index_compra_atual]['Preço']\n",
    "        quantidade_compra = self.purchases.iloc[index_compra_atual]['Quantidade']\n",
    "\n",
    "        if quantidade_compra > 100:\n",
    "          quantidade_compra /= 100\n",
    "          if quantidade_compra > 100:\n",
    "            quantidade_compra /= 10\n",
    "\n",
    "        if self.current_step == 0:\n",
    "          preco_medio_at = preco_compra\n",
    "        else:\n",
    "            preco_medio_at = calculate_current_price(self.all_quantities[self.current_episode], self.purchases)\n",
    "\n",
    "        purchase_combined_quantity = self.all_quantities[self.current_episode][index_compra_atual]\n",
    "        if purchase_combined_quantity > 100:\n",
    "          purchase_combined_quantity /= 100\n",
    "          if purchase_combined_quantity > 100:\n",
    "            purchase_combined_quantity /= 10\n",
    "\n",
    "        self.max_steps = quantidade_linhas_compra * max_steps\n",
    "\n",
    "        return np.array([preco_venda, quantidade_venda, preco_medio_at, preco_compra, quantidade_compra, preco_ideal, purchase_combined_quantity]).reshape(len(observation_data),)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87641de1",
   "metadata": {},
   "source": [
    "# Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0347ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros\n",
    "max_steps = 5\n",
    "learning_rate = 0.01\n",
    "nb_steps = 1000000\n",
    "neurons_hidden_layer = 24\n",
    "hidden_layer_activation = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1654a271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-rl2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras-rl2) (2.15.0.post1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (69.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow->keras-rl2) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging->tensorflow->keras-rl2) (3.1.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->keras-rl2) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c9a39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import __version__\n",
    "tf.keras.__version__ = __version__\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.callbacks import Callback\n",
    "\n",
    "env = Environment()\n",
    "states = (len(observation_data), )\n",
    "actions = env.action_space.n\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1, len(observation_data))))\n",
    "    model.add(Dense(neurons_hidden_layer, activation=hidden_layer_activation, input_shape=states))\n",
    "    model.add(Dense(neurons_hidden_layer, activation=hidden_layer_activation))\n",
    "    model.add(Dense(neurons_hidden_layer, activation=hidden_layer_activation))\n",
    "    model.add(Dense(actions, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9351eac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 7)                 0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 24)                192       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 21)                525       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1917 (7.49 KB)\n",
      "Trainable params: 1917 (7.49 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b836d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class PlotRewardsCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super(PlotRewardsCallback, self).__init__()\n",
    "        # Initialize lists to store episode numbers and rewards\n",
    "        self.episodes = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        # Append episode number and reward\n",
    "        self.episodes.append(episode)\n",
    "        self.rewards.append(logs['episode_reward'])\n",
    "\n",
    "        # Plot or update the plot\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.episodes, self.rewards, marker='o', linestyle='-')\n",
    "        plt.title('Episode Rewards')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8de136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 16:04:03.932951: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_6_1/kernel/Assign' id:983 op device:{requested: '', assigned: ''} def:{{{node dense_6_1/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_6_1/kernel, dense_6_1/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2024-03-29 16:04:04.309907: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_7/Softmax' id:919 op device:{requested: '', assigned: ''} def:{{{node dense_7/Softmax}} = Softmax[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_7/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-03-29 16:04:04.374550: W tensorflow/c/c_api.cc:305] Operation '{name:'count_5/Assign' id:1186 op device:{requested: '', assigned: ''} def:{{{node count_5/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_5, count_5/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100/10000 [..............................] - ETA: 2:54 - reward: -382.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 16:04:06.317046: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_7_1/Softmax' id:1018 op device:{requested: '', assigned: ''} def:{{{node dense_7_1/Softmax}} = Softmax[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_7_1/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-03-29 16:04:06.530568: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_7/AddN' id:1301 op device:{requested: '', assigned: ''} def:{{{node loss_7/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_7/mul, loss_7/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-03-29 16:04:06.595626: W tensorflow/c/c_api.cc:305] Operation '{name:'training_2/Adam/dense_6/bias/m/Assign' id:1479 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/dense_6/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/dense_6/bias/m, training_2/Adam/dense_6/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 260s 26ms/step - reward: -380.4834\n",
      "48 episodes - episode_reward: -78240.283 [-133990.000, 1000.000] - loss: 201450.492 - mse: 19185.802 - mean_q: 1.000\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 290s 29ms/step - reward: -377.6576\n",
      "45 episodes - episode_reward: -84999.467 [-288790.000, -29800.000] - loss: 190502.875 - mse: 18143.162 - mean_q: 1.000\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: -330.6263\n",
      "51 episodes - episode_reward: -64297.310 [-113290.000, 1200.000] - loss: 187995.938 - mse: 17904.398 - mean_q: 1.000\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 272s 27ms/step - reward: -293.0785\n",
      "45 episodes - episode_reward: -64955.227 [-269290.000, -3090.327] - loss: 177327.656 - mse: 16888.412 - mean_q: 1.000\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: -276.2530\n",
      "55 episodes - episode_reward: -50595.089 [-111090.000, 1100.000] - loss: 170935.875 - mse: 16279.672 - mean_q: 1.000\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 303s 30ms/step - reward: -263.6190\n",
      "46 episodes - episode_reward: -56519.350 [-241390.000, 1409.881] - loss: 159557.969 - mse: 15196.192 - mean_q: 1.000\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 262s 26ms/step - reward: -240.9534\n",
      "56 episodes - episode_reward: -43936.323 [-111190.000, 1100.000] - loss: 148350.156 - mse: 14128.657 - mean_q: 1.000\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 296s 30ms/step - reward: -239.5384\n",
      "47 episodes - episode_reward: -50703.915 [-253090.000, 900.000] - loss: 136593.375 - mse: 13008.944 - mean_q: 1.000\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 283s 28ms/step - reward: -213.8649\n",
      "56 episodes - episode_reward: -37988.383 [-104990.000, 1100.000] - loss: 129497.688 - mse: 12333.117 - mean_q: 1.000\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 298s 30ms/step - reward: -209.4695\n",
      "47 episodes - episode_reward: -43659.471 [-236090.000, 1100.000] - loss: 121083.586 - mse: 11531.841 - mean_q: 1.000\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: -201.3250\n",
      "59 episodes - episode_reward: -34500.841 [-106190.000, 1100.000] - loss: 115897.688 - mse: 11037.914 - mean_q: 1.000\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 301s 30ms/step - reward: -178.6578\n",
      "45 episodes - episode_reward: -40439.516 [-242390.000, 0.000] - loss: 109112.445 - mse: 10391.720 - mean_q: 1.000\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: -194.0036\n",
      "59 episodes - episode_reward: -32997.225 [-108090.000, 1200.000] - loss: 105294.828 - mse: 10028.136 - mean_q: 1.000\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 305s 30ms/step - reward: -188.1697\n",
      "45 episodes - episode_reward: -40426.590 [-222890.000, 0.000] - loss: 99150.531 - mse: 9442.979 - mean_q: 1.000\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 283s 28ms/step - reward: -181.5348\n",
      "59 episodes - episode_reward: -31827.939 [-112990.000, 1100.000] - loss: 97568.703 - mse: 9292.277 - mean_q: 1.000\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 314s 31ms/step - reward: -172.2779\n",
      "51 episodes - episode_reward: -33576.053 [-237490.000, 1100.000] - loss: 95162.336 - mse: 9063.119 - mean_q: 1.000\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 299s 30ms/step - reward: -163.3043\n",
      "54 episodes - episode_reward: -29924.873 [-114990.000, 1100.000] - loss: 92857.258 - mse: 8843.590 - mean_q: 1.000\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 328s 33ms/step - reward: -169.3181\n",
      "52 episodes - episode_reward: -32997.710 [-228990.000, 1100.000] - loss: 90632.102 - mse: 8631.664 - mean_q: 1.000\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 317s 32ms/step - reward: -152.5865\n",
      "54 episodes - episode_reward: -28201.195 [-105990.000, 1100.000] - loss: 88970.016 - mse: 8473.380 - mean_q: 1.000\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 310s 31ms/step - reward: -173.4476\n",
      "51 episodes - episode_reward: -32532.858 [-219790.000, 1100.000] - loss: 86326.320 - mse: 8221.598 - mean_q: 1.000\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 307s 31ms/step - reward: -132.7158\n",
      "55 episodes - episode_reward: -24968.326 [-100190.000, 1100.000] - loss: 84537.883 - mse: 8051.272 - mean_q: 1.000\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 294s 29ms/step - reward: -162.5875\n",
      "51 episodes - episode_reward: -32521.082 [-198890.000, 1100.000] - loss: 82448.227 - mse: 7852.279 - mean_q: 1.000\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 308s 31ms/step - reward: -147.6182\n",
      "53 episodes - episode_reward: -27463.808 [-215090.000, 1100.000] - loss: 80464.578 - mse: 7663.325 - mean_q: 1.000\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: -151.0549\n",
      "57 episodes - episode_reward: -26934.200 [-108090.000, 1100.000] - loss: 79341.586 - mse: 7556.361 - mean_q: 1.000\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 298s 30ms/step - reward: -141.8302\n",
      "49 episodes - episode_reward: -28236.784 [-198390.000, 1100.000] - loss: 76737.820 - mse: 7308.410 - mean_q: 1.000\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 269s 27ms/step - reward: -139.0043\n",
      "59 episodes - episode_reward: -23876.992 [-90990.000, 1100.000] - loss: 76018.352 - mse: 7239.889 - mean_q: 1.000\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 291s 29ms/step - reward: -142.3293\n",
      "49 episodes - episode_reward: -29493.736 [-187590.000, 900.000] - loss: 74769.547 - mse: 7120.946 - mean_q: 1.000\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 271s 27ms/step - reward: -128.6846\n",
      "60 episodes - episode_reward: -21274.104 [-110090.000, 1100.000] - loss: 73883.844 - mse: 7036.579 - mean_q: 1.000\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 291s 29ms/step - reward: -144.1984\n",
      "50 episodes - episode_reward: -28619.679 [-181590.000, 1100.000] - loss: 71690.680 - mse: 6827.745 - mean_q: 1.000\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 275s 27ms/step - reward: -119.4355\n",
      "59 episodes - episode_reward: -20429.749 [-92590.000, 1100.000] - loss: 71944.062 - mse: 6851.871 - mean_q: 1.000\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 303s 30ms/step - reward: -136.1984\n",
      "53 episodes - episode_reward: -25826.117 [-199190.000, 1100.000] - loss: 69142.414 - mse: 6585.021 - mean_q: 1.000\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 278s 28ms/step - reward: -112.5081\n",
      "56 episodes - episode_reward: -20019.306 [-110990.000, 1100.000] - loss: 69132.938 - mse: 6584.133 - mean_q: 1.000\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 295s 30ms/step - reward: -144.0682\n",
      "53 episodes - episode_reward: -27312.860 [-192390.000, 1100.000] - loss: 68803.859 - mse: 6552.793 - mean_q: 1.000\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 304s 30ms/step - reward: -122.7110\n",
      "53 episodes - episode_reward: -20330.380 [-116090.000, 1100.000] - loss: 67344.883 - mse: 6413.851 - mean_q: 1.000\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 291s 29ms/step - reward: -142.8839\n",
      "57 episodes - episode_reward: -27644.537 [-180790.000, 1100.000] - loss: 67214.602 - mse: 6401.423 - mean_q: 1.000\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 306s 31ms/step - reward: -119.1992\n",
      "51 episodes - episode_reward: -22866.505 [-168890.000, 900.000] - loss: 66597.094 - mse: 6342.629 - mean_q: 1.000\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 287s 29ms/step - reward: -118.0948\n",
      "60 episodes - episode_reward: -20097.461 [-109990.000, 1100.000] - loss: 66882.727 - mse: 6369.826 - mean_q: 1.000\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 301s 30ms/step - reward: -119.7888\n",
      "50 episodes - episode_reward: -24029.755 [-146690.000, 900.000] - loss: 65366.055 - mse: 6225.383 - mean_q: 1.000\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 271s 27ms/step - reward: -119.9742\n",
      "61 episodes - episode_reward: -19653.140 [-100090.000, 1510.000] - loss: 66043.133 - mse: 6289.863 - mean_q: 1.000\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 296s 30ms/step - reward: -128.6184\n",
      "51 episodes - episode_reward: -25038.895 [-168690.000, 1100.000] - loss: 63848.230 - mse: 6080.846 - mean_q: 1.000\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 285s 28ms/step - reward: -118.4152\n",
      "56 episodes - episode_reward: -20977.717 [-108990.000, 1100.000] - loss: 64271.258 - mse: 6121.136 - mean_q: 1.000\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 300s 30ms/step - reward: -129.5472\n",
      "53 episodes - episode_reward: -24442.870 [-134090.000, 1100.000] - loss: 63668.719 - mse: 6063.731 - mean_q: 1.000\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 283s 28ms/step - reward: -111.9469\n",
      "57 episodes - episode_reward: -19711.731 [-103090.000, 1100.000] - loss: 64674.398 - mse: 6159.510 - mean_q: 1.000\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 295s 30ms/step - reward: -135.6477\n",
      "52 episodes - episode_reward: -26076.478 [-129090.000, 1100.000] - loss: 64354.746 - mse: 6129.066 - mean_q: 1.000\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 291s 29ms/step - reward: -106.1973\n",
      "53 episodes - episode_reward: -20227.794 [-133090.000, 1100.000] - loss: 64519.379 - mse: 6144.751 - mean_q: 1.000\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 274s 27ms/step - reward: -124.6359\n",
      "58 episodes - episode_reward: -21321.715 [-102190.000, 1100.000] - loss: 63365.055 - mse: 6034.812 - mean_q: 1.000\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 311s 31ms/step - reward: -108.2388\n",
      "50 episodes - episode_reward: -21897.767 [-140000.000, 1100.000] - loss: 62150.316 - mse: 5919.115 - mean_q: 1.000\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 271s 27ms/step - reward: -113.0241\n",
      "60 episodes - episode_reward: -18852.344 [-99090.000, 1100.000] - loss: 61578.680 - mse: 5864.675 - mean_q: 1.000\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 288s 29ms/step - reward: -113.7571\n",
      "53 episodes - episode_reward: -19995.682 [-113090.000, 1100.000] - loss: 59318.398 - mse: 5649.422 - mean_q: 1.000\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 269s 27ms/step - reward: -111.5530\n",
      "61 episodes - episode_reward: -19598.852 [-119090.000, 1100.000] - loss: 59733.863 - mse: 5688.974 - mean_q: 1.000\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 272s 27ms/step - reward: -115.0278\n",
      "56 episodes - episode_reward: -20231.759 [-109090.000, 1100.000] - loss: 59313.363 - mse: 5648.936 - mean_q: 1.000\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: -110.4554\n",
      "56 episodes - episode_reward: -19877.748 [-111190.000, 1100.000] - loss: 60082.695 - mse: 5722.212 - mean_q: 1.000\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 287s 29ms/step - reward: -123.0145\n",
      "59 episodes - episode_reward: -20995.681 [-117090.000, 1100.000] - loss: 60314.965 - mse: 5744.315 - mean_q: 1.000\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 319s 32ms/step - reward: -110.3475\n",
      "52 episodes - episode_reward: -21070.665 [-111990.000, 1100.000] - loss: 60690.258 - mse: 5780.064 - mean_q: 1.000\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: -117.6851\n",
      "58 episodes - episode_reward: -20409.498 [-107190.000, 1100.000] - loss: 60349.383 - mse: 5747.608 - mean_q: 1.000\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 290s 29ms/step - reward: -106.7076\n",
      "54 episodes - episode_reward: -19501.417 [-114090.000, 1100.000] - loss: 60088.090 - mse: 5722.713 - mean_q: 1.000\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 284s 28ms/step - reward: -124.4247\n",
      "60 episodes - episode_reward: -20654.116 [-113190.000, 1100.000] - loss: 60192.613 - mse: 5732.671 - mean_q: 1.000\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 288s 29ms/step - reward: -119.5358\n",
      "55 episodes - episode_reward: -21971.959 [-109890.000, 1100.000] - loss: 60657.836 - mse: 5776.990 - mean_q: 1.000\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 280s 28ms/step - reward: -111.1257\n",
      "56 episodes - episode_reward: -19729.586 [-108090.000, 1100.000] - loss: 61393.805 - mse: 5847.082 - mean_q: 1.000\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 282s 28ms/step - reward: -121.8367\n",
      "56 episodes - episode_reward: -21874.404 [-105990.000, 1100.000] - loss: 60390.117 - mse: 5751.477 - mean_q: 1.000\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 272s 27ms/step - reward: -101.7261\n",
      "54 episodes - episode_reward: -18456.686 [-115990.000, 1100.000] - loss: 60908.000 - mse: 5800.783 - mean_q: 1.000\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 276s 28ms/step - reward: -129.6642\n",
      "58 episodes - episode_reward: -22697.282 [-102190.000, 1100.000] - loss: 59952.008 - mse: 5709.766 - mean_q: 1.000\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 285s 28ms/step - reward: -101.7270\n",
      "54 episodes - episode_reward: -18958.713 [-102090.000, 1304.776] - loss: 60366.523 - mse: 5749.228 - mean_q: 1.000\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 279s 28ms/step - reward: -122.9241\n",
      "60 episodes - episode_reward: -19794.024 [-103090.000, 1100.000] - loss: 60191.297 - mse: 5732.539 - mean_q: 1.000\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 282s 28ms/step - reward: -117.5772\n",
      "54 episodes - episode_reward: -22308.743 [-119090.000, 1100.000] - loss: 60851.316 - mse: 5795.420 - mean_q: 1.000\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 262s 26ms/step - reward: -117.9960\n",
      "59 episodes - episode_reward: -20109.494 [-100090.000, 1100.000] - loss: 61776.871 - mse: 5883.567 - mean_q: 1.000\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 266s 27ms/step - reward: -113.9265\n",
      "55 episodes - episode_reward: -20612.084 [-103990.000, 1100.000] - loss: 60189.992 - mse: 5732.413 - mean_q: 1.000\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 260s 26ms/step - reward: -100.1076\n",
      "56 episodes - episode_reward: -18040.636 [-100090.000, 1100.000] - loss: 61223.898 - mse: 5830.898 - mean_q: 1.000\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 277s 28ms/step - reward: -122.9856\n",
      "58 episodes - episode_reward: -21142.344 [-100090.000, 1100.000] - loss: 59557.301 - mse: 5672.161 - mean_q: 1.000\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: -104.6869\n",
      "54 episodes - episode_reward: -19458.692 [-110090.000, 1100.000] - loss: 59391.988 - mse: 5656.424 - mean_q: 1.000\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 262s 26ms/step - reward: -115.1550\n",
      "61 episodes - episode_reward: -18886.071 [-104090.000, 1100.000] - loss: 58386.367 - mse: 5560.663 - mean_q: 1.000\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: -112.0268\n",
      "54 episodes - episode_reward: -20667.933 [-108090.000, 1100.000] - loss: 58790.648 - mse: 5599.158 - mean_q: 1.000\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 256s 26ms/step - reward: -113.7545\n",
      "61 episodes - episode_reward: -18740.076 [-107090.000, 1100.000] - loss: 58922.105 - mse: 5611.674 - mean_q: 1.000\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 262s 26ms/step - reward: -104.7359\n",
      "56 episodes - episode_reward: -18726.050 [-102090.000, 1100.000] - loss: 58276.543 - mse: 5550.185 - mean_q: 1.000\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: -100.0168\n",
      "55 episodes - episode_reward: -18053.973 [-101090.000, 1100.000] - loss: 58078.133 - mse: 5531.301 - mean_q: 1.000\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 265s 27ms/step - reward: -124.8938\n",
      "59 episodes - episode_reward: -21163.359 [-101990.000, 1100.000] - loss: 58103.691 - mse: 5533.729 - mean_q: 1.000\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 267s 27ms/step - reward: -117.5454\n",
      "53 episodes - episode_reward: -22276.491 [-114990.000, 1100.000] - loss: 58767.770 - mse: 5596.965 - mean_q: 1.000\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 255s 26ms/step - reward: -105.0046\n",
      "61 episodes - episode_reward: -17235.177 [-109990.000, 1100.000] - loss: 58159.438 - mse: 5539.032 - mean_q: 1.000\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 257s 26ms/step - reward: -111.1173\n",
      "52 episodes - episode_reward: -20586.027 [-106990.000, 900.000] - loss: 58362.152 - mse: 5558.324 - mean_q: 1.000\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 249s 25ms/step - reward: -108.5435\n",
      "62 episodes - episode_reward: -18168.299 [-92190.000, 1100.000] - loss: 58969.395 - mse: 5616.174 - mean_q: 1.000\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 238s 24ms/step - reward: -116.8756\n",
      "55 episodes - episode_reward: -21111.919 [-117990.000, 1100.000] - loss: 58248.168 - mse: 5547.499 - mean_q: 1.000\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 261s 26ms/step - reward: -106.4959\n",
      "56 episodes - episode_reward: -18788.556 [-103990.000, 1100.000] - loss: 59058.387 - mse: 5624.658 - mean_q: 1.000\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 261s 26ms/step - reward: -125.9357\n",
      "57 episodes - episode_reward: -22411.526 [-108190.000, 1100.000] - loss: 58627.211 - mse: 5583.592 - mean_q: 1.000\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 256s 26ms/step - reward: -114.7867\n",
      "55 episodes - episode_reward: -19930.304 [-111990.000, 1100.000] - loss: 59832.168 - mse: 5698.331 - mean_q: 1.000\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 257s 26ms/step - reward: -122.9848\n",
      "59 episodes - episode_reward: -21592.336 [-109190.000, 1100.000] - loss: 60548.621 - mse: 5766.560 - mean_q: 1.000\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 261s 26ms/step - reward: -107.8876\n",
      "54 episodes - episode_reward: -20108.812 [-111090.000, 1100.000] - loss: 61353.352 - mse: 5843.210 - mean_q: 1.000\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 249s 25ms/step - reward: -112.0730\n",
      "62 episodes - episode_reward: -17797.257 [-113190.000, 1200.000] - loss: 60258.938 - mse: 5738.963 - mean_q: 1.000\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 250s 25ms/step - reward: -117.5069\n",
      "55 episodes - episode_reward: -21473.990 [-110090.000, 1100.000] - loss: 60445.566 - mse: 5756.773 - mean_q: 1.000\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 252s 25ms/step - reward: -106.8363\n",
      "58 episodes - episode_reward: -18601.084 [-116090.000, 1100.000] - loss: 60768.844 - mse: 5787.540 - mean_q: 1.000\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 258s 26ms/step - reward: -124.6148\n",
      "56 episodes - episode_reward: -21868.713 [-104990.000, 1100.000] - loss: 59704.512 - mse: 5686.195 - mean_q: 1.000\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 256s 26ms/step - reward: -112.3646\n",
      "57 episodes - episode_reward: -20118.358 [-130090.000, 1100.000] - loss: 59764.332 - mse: 5691.875 - mean_q: 1.000\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 259s 26ms/step - reward: -113.2733\n",
      "59 episodes - episode_reward: -19192.083 [-114990.000, 1100.000] - loss: 59849.215 - mse: 5699.974 - mean_q: 1.000\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 265s 26ms/step - reward: -108.5873\n",
      "55 episodes - episode_reward: -19339.518 [-95090.000, 1100.000] - loss: 59642.457 - mse: 5680.269 - mean_q: 1.000\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 254s 25ms/step - reward: -111.3648\n",
      "61 episodes - episode_reward: -18359.805 [-102990.000, 1100.000] - loss: 59719.602 - mse: 5687.641 - mean_q: 1.000\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 263s 26ms/step - reward: -115.2259\n",
      "57 episodes - episode_reward: -20450.155 [-117090.000, 1100.000] - loss: 58985.062 - mse: 5617.663 - mean_q: 1.000\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 265s 26ms/step - reward: -98.3755\n",
      "57 episodes - episode_reward: -17209.742 [-108990.000, 1100.000] - loss: 58460.863 - mse: 5567.731 - mean_q: 1.000\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 250s 25ms/step - reward: -124.2348\n",
      "59 episodes - episode_reward: -21138.095 [-105090.000, 1100.000] - loss: 58197.523 - mse: 5542.661 - mean_q: 1.000\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 264s 26ms/step - reward: -118.4972\n",
      "54 episodes - episode_reward: -21938.375 [-120990.000, 1100.000] - loss: 59288.398 - mse: 5646.573 - mean_q: 1.000\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 249s 25ms/step - reward: -111.0449\n",
      "60 episodes - episode_reward: -18339.142 [-103090.000, 1100.000] - loss: 59908.590 - mse: 5705.622 - mean_q: 1.000\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 269s 27ms/step - reward: -113.3374\n",
      "done, took 27842.240 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 9.94169376e+01, -4.66013111e+02, -3.50782554e+01, -2.32025458e+02,\n",
       "        1.98025789e+02, -1.73241714e+02,  1.02986275e+02,  1.03041608e+02,\n",
       "       -2.25301184e+02,  1.03016322e+02,  1.00582081e+02, -6.28437787e+01,\n",
       "       -1.46374917e+02,  3.21765136e-01, -4.91408008e+02, -6.24507622e+01,\n",
       "       -5.38455564e+02,  1.03239368e+02, -6.02126519e+02, -5.57578031e+01,\n",
       "       -5.51722484e+02, -5.70175383e+01,  1.03830771e+02, -1.48255127e+02,\n",
       "       -3.87368709e+02, -5.60685046e+02, -7.81467485e+01, -8.83846718e+01,\n",
       "       -1.20564335e+02, -3.38000497e+01, -3.77261798e+02, -6.86477253e+02,\n",
       "        1.00945232e+02,  1.00514897e+02, -5.74298601e+01, -3.46897826e+02,\n",
       "       -5.13379762e+02, -1.89762059e+02, -3.09660803e+01, -1.96616974e+02,\n",
       "       -1.55514945e+02, -1.54797507e+01, -1.15894213e+02,  1.00253014e+02,\n",
       "       -1.56511587e+02, -5.04638040e+02,  2.10772560e+01,  1.02265589e+02,\n",
       "       -4.63024202e+02, -9.26787118e+01, -2.07881050e+02, -2.88718802e+02,\n",
       "        1.03056819e+02, -4.63414511e+02,  1.00919618e+02, -4.97318826e+02,\n",
       "       -1.97233482e+02, -4.40830685e+02, -7.30386397e+01,  1.02283111e+02,\n",
       "       -1.37266081e+02, -4.11740727e+02, -2.26250454e+02, -8.47444193e+01,\n",
       "       -2.37779397e+02, -4.73137972e+01, -1.39996674e+02, -6.79391517e+01,\n",
       "       -2.70735167e+01,  1.02910358e+02, -2.44575932e+01, -6.25202939e+02,\n",
       "       -1.65443120e+02,  1.02861842e+02, -1.78958694e+02,  9.78456750e+02,\n",
       "       -6.06116680e+01, -2.25606456e+02, -2.41337609e+02, -3.59631781e+02,\n",
       "       -3.44680616e+02, -2.47147626e+02,  1.02398655e+02, -4.24013381e+02,\n",
       "       -7.27452489e+01, -9.37608904e+00, -5.68716614e+02, -3.15403503e+02,\n",
       "       -2.23599614e+02, -4.30433994e+02, -3.03567865e+02, -2.20977725e+02,\n",
       "       -4.06742512e+02, -7.93172802e+01,  1.00438919e+02,  1.19632281e+01,\n",
       "       -6.81947904e+02,  1.03598478e+02, -3.76434911e+01, -5.91671990e+02])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate), metrics=['mse'])\n",
    "plot_rewards_callback = PlotRewardsCallback()\n",
    "dqn.fit(env, nb_steps=nb_steps, visualize=True, verbose=1, nb_max_episode_steps=None)\n",
    "env.cdis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e95fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [preco_venda, quantidade_venda, preco_medio_at, preco_compra, quantidade_compra, preco_ideal, purchase_combined_quantity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c81dac57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acertos: 24.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = 0\n",
    "for i in range(len(env.best_cdis)):\n",
    "  if env.best_cdis[i]:\n",
    "    count += 1\n",
    "\n",
    "print(f\"Acertos: {(count / len(env.cdis)) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6024d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
