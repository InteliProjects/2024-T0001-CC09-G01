{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7daac5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7daac5f",
        "outputId": "4995609a-fd17-4d32-b896-81ab43e60b81"
      },
      "outputs": [],
      "source": [
        "!{sys.executable} -m pip install stable-baselines3\n",
        "!{sys.executable} -m pip install shimmy>=0.2.1\n",
        "!{sys.executable} -m pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mEHeG3u2NxRB",
      "metadata": {
        "id": "mEHeG3u2NxRB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from gym import Env\n",
        "from gym import spaces\n",
        "from collections import deque\n",
        "from tabulate import tabulate\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0aUhY7OjnBD",
      "metadata": {
        "id": "c0aUhY7OjnBD"
      },
      "outputs": [],
      "source": [
        "with open('dataframes_train.txt', 'rb') as f:\n",
        "    dataframes_train = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XxIgahLZ7xC-",
      "metadata": {
        "id": "XxIgahLZ7xC-"
      },
      "outputs": [],
      "source": [
        "with open('dataframes_test.txt', 'rb') as f:\n",
        "    dataframes_test = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b92b4918",
      "metadata": {
        "id": "b92b4918"
      },
      "source": [
        "# Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uR8PL-p2Jwxn",
      "metadata": {
        "id": "uR8PL-p2Jwxn"
      },
      "outputs": [],
      "source": [
        "ob_data = [\n",
        "    \"qtd disponivel de venda\",\n",
        "    \"qtd disponivel de compra\",\n",
        "    \"delta preco ideal - atual\",\n",
        "    \"qtd casada\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08YXZ-yLMXIz",
      "metadata": {
        "id": "08YXZ-yLMXIz"
      },
      "outputs": [],
      "source": [
        "class AuxFunctions():\n",
        "  \"\"\"\n",
        "  Class with auxiliary functions used in the environment\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def calculate_cdi(self, sale_price, purchase_price, du, di):\n",
        "    \"\"\"Calculates the CDI for the combined purchases\n",
        "        Args:\n",
        "            sale_price (float): The price of the sale\n",
        "            purchase_price (float): The average price of the combined purchases\n",
        "            du (int): The number of business days between the sale operation date and the expiration date\n",
        "            di (float): The DI tax of the day\n",
        "\n",
        "        Returns:\n",
        "            float: The CDI for the combined purchases\n",
        "    \"\"\"\n",
        "    if purchase_price == 0:\n",
        "      return 0\n",
        "    rent = ((sale_price / purchase_price) - 1)\n",
        "    annual_rent = ((1 + rent) ** (252 / du)) - 1\n",
        "\n",
        "    cdi = (annual_rent / di) * 100\n",
        "\n",
        "    return cdi\n",
        "\n",
        "  def calculate_ideal_price(self, sale_price, du, di):\n",
        "    \"\"\"Calculates the ideal average price for the combined purchases\n",
        "        Uses the formula: pi=pv/(di+1)^du/252\n",
        "        where:\n",
        "        pi = ideal price\n",
        "        pv = sale price\n",
        "        du = business days\n",
        "        di = DI tax of the day\n",
        "\n",
        "      Args:\n",
        "          sale_price (float): the mean price of the sale operation\n",
        "          du (int): business days\n",
        "          di (float): DI tax of the day\n",
        "\n",
        "      Returns:\n",
        "          float: ideal average price for the combined purchases\n",
        "      \"\"\"\n",
        "    return sale_price / ((di + 1) ** (du / 252))\n",
        "\n",
        "  def calculate_average_price(self, quantities, purchases):\n",
        "    \"\"\"Calculates the current average price for the combined purchases\n",
        "\n",
        "      Args:\n",
        "        quantities (list): an array with the combined quantity for each purchase\n",
        "        purchases (dataframe): all purchases for a specific sale\n",
        "\n",
        "      Returns:\n",
        "        float: The new average price for the combined purchases\n",
        "    \"\"\"\n",
        "    total_price = 0\n",
        "    total_quantity = 0\n",
        "\n",
        "    for purchase_index in range(len(quantities)):\n",
        "        total_price += purchases.iloc[purchase_index][\"Preço\"] * quantities[purchase_index]\n",
        "        total_quantity += quantities[purchase_index]\n",
        "\n",
        "    if total_quantity == 0:\n",
        "      return 0\n",
        "    return total_price / total_quantity\n",
        "\n",
        "  def update_dataframes(self, current_sale_index, original_dataframes, dataframes, purchases_quantities):\n",
        "    \"\"\"\n",
        "    Updates the purchases quantities for the sales that share the same purchases as the one combined in the moment.\n",
        "\n",
        "    Args:\n",
        "        current_sale_index (int): The index of the sale combined in the moment\n",
        "        original_dataframes (list): The original dataframes with all the sales and respective purchases\n",
        "        dataframes (list): The dataframes updated with all combinations already made\n",
        "        purchases_quantities (list): The quantities of the combined purchases\n",
        "\n",
        "    Returns:\n",
        "        dataframes (list): The dataframes updated with the new quantities of the combined purchases\n",
        "    \"\"\"\n",
        "    current_sale = dataframes[current_sale_index]['sale']\n",
        "\n",
        "    # Check if there are other sales with the same purchases before the current sale\n",
        "    first_index = current_sale_index\n",
        "    for sale_index in range(current_sale_index, 0, -1):\n",
        "      sale = original_dataframes[sale_index]['sale']\n",
        "\n",
        "      criteria = (sale['Cód. Cliente'] == current_sale['Cód. Cliente'] and\n",
        "                  sale['Dt. Operação'] == current_sale['Dt. Operação'] and\n",
        "                  sale['Cód. Título'] == current_sale['Cód. Título'] and\n",
        "                  sale['Cód. Corretora'] == current_sale['Cód. Corretora'])\n",
        "\n",
        "      if not criteria:\n",
        "          break\n",
        "      else:\n",
        "        first_index -= 1\n",
        "\n",
        "\n",
        "    # Check if there are other sales with the same purchases after the current sale\n",
        "    for sale_index in range(first_index + 1, len(dataframes)):\n",
        "        sale = original_dataframes[sale_index]['sale']\n",
        "\n",
        "        criteria = (sale['Cód. Cliente'] == current_sale['Cód. Cliente'] and\n",
        "                    sale['Dt. Operação'] == current_sale['Dt. Operação'] and\n",
        "                    sale['Cód. Título'] == current_sale['Cód. Título'] and\n",
        "                    sale['Cód. Corretora'] == current_sale['Cód. Corretora'])\n",
        "\n",
        "        if not criteria:\n",
        "            break\n",
        "\n",
        "        purchases = dataframes[sale_index]['purchase']\n",
        "        for i in range(len(purchases_quantities)):\n",
        "          purchases.iloc[i, purchases.columns.get_loc(\"Quantidade\")] -= purchases_quantities[i]\n",
        "\n",
        "    return dataframes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n_IuT8SzFYxb",
      "metadata": {
        "id": "n_IuT8SzFYxb"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "class Matching(Env):\n",
        "  \"\"\" Implements the environment for an RL agent to learn how to combine purchases and sales to maximize the CDI around 100%\n",
        "\n",
        "  Args:\n",
        "      Env (object): OpenAI Gym environment as base class\n",
        "  \"\"\"\n",
        "  def __init__(self, dataframes):\n",
        "    self.actions = [1, 0.5, 0, -0.5, -1]\n",
        "    self.action_space = spaces.Discrete(len(self.actions))\n",
        "    self.observation_space = spaces.Box(0, np.inf, shape=(len(ob_data),))\n",
        "    self.utils = AuxFunctions()\n",
        "    self.original_data = deepcopy(dataframes)\n",
        "    self.data = deepcopy(dataframes)\n",
        "    self.all_quantities_matched = [np.zeros(x['purchase'].shape[0]) for x in self.original_data]\n",
        "    self.du = 0\n",
        "    self.di = 0\n",
        "    self.current_episode = 0\n",
        "    self.cdis = np.zeros(len(self.data))\n",
        "    self.state = None\n",
        "    self.combinations = {}\n",
        "    self.none_actions = 0\n",
        "    self.max_none_actions = 3\n",
        "\n",
        "  def update_data(self, action):\n",
        "      \"\"\"Updates the quantities for the purchases and sales based on the agent's action\n",
        "\n",
        "      Args:\n",
        "          action (int): The percentage of the purchase quantity to be combined or discombined (positive or negative respectively)\n",
        "\n",
        "      Returns:\n",
        "          int: The actual quantity that can be combined or discombined\n",
        "      \"\"\"\n",
        "      current_purchase_index = self.current_step % self.purchases.shape[0]\n",
        "\n",
        "      if action < 0:\n",
        "        quantity_of_purchase = int(action * self.all_quantities_matched[self.current_episode][current_purchase_index])\n",
        "      else:\n",
        "        quantity_of_purchase = int(action * self.purchases.iloc[current_purchase_index]['Quantidade'])\n",
        "\n",
        "\n",
        "      if quantity_of_purchase < 0:\n",
        "        if abs(quantity_of_purchase) > self.all_quantities_matched[self.current_episode][current_purchase_index]:\n",
        "          quantity_of_purchase = -1 * self.all_quantities_matched[self.current_episode][current_purchase_index]\n",
        "\n",
        "      if quantity_of_purchase - self.sale[\"Quantidade\"] > 0:\n",
        "        quantity_of_purchase = self.sale[\"Quantidade\"]\n",
        "\n",
        "      self.all_quantities_matched[self.current_episode][current_purchase_index] += quantity_of_purchase\n",
        "\n",
        "      self.sale[\"Quantidade\"] = self.sale[\"Quantidade\"] - quantity_of_purchase\n",
        "      self.purchases.iloc[current_purchase_index, self.purchases.columns.get_loc(\"Quantidade\")] -= quantity_of_purchase\n",
        "      return quantity_of_purchase\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Executes the action and updates the environment state by updating the quantities and prices of the purchases and sales, and calculating the reward and CDI\n",
        "        The stop conditions are if the total sale quantity was used, or if the maximum number of steps is reached\n",
        "\n",
        "    Args:\n",
        "        action (_type_): The index of the action to be executed\n",
        "\n",
        "    Returns:\n",
        "        state (list): The new state of the environment\n",
        "        reward (int): The reward for the agent\n",
        "        done (bool): Whether the episode is done or not\n",
        "        info (dict): Additional information for debugging\n",
        "    \"\"\"\n",
        "    done = False\n",
        "    reward = 0\n",
        "    quantity = self.update_data(self.actions[action])\n",
        "\n",
        "    if quantity == 0:\n",
        "      self.none_actions += 1\n",
        "    else:\n",
        "      self.none_actions = 0\n",
        "\n",
        "    if self.combinations.get(self.current_episode) is not None and self.combinations[self.current_episode].size > 0:\n",
        "      self.all_quantities_matched[self.current_episode] = deepcopy(self.combinations[self.current_episode])\n",
        "\n",
        "    current_average_price = self.utils.calculate_average_price(self.all_quantities_matched[self.current_episode], self.purchases)\n",
        "\n",
        "    old_distance = self.state[2]\n",
        "\n",
        "    self.state = self.get_state()\n",
        "\n",
        "    # Reward based on how close the average price is to the ideal price compared to the previous state\n",
        "    new_distance = self.state[2]\n",
        "    if new_distance < old_distance and current_average_price <= self.sale[\"Preço\"]:\n",
        "      if new_distance != 0:\n",
        "        reward +=  500 / new_distance\n",
        "    else:\n",
        "      reward -= 500\n",
        "\n",
        "    # Reward based on the agent doing nothing for a long time\n",
        "    if self.none_actions == self.max_none_actions:\n",
        "      reward -= 250\n",
        "\n",
        "    # Reward based on the agent using all the sale quantity and the CDI being close to 100%\n",
        "    # If the CDI is between 95% and 110%, the agent receives a high reward\n",
        "    # If the CDI is below 0, the agent receives a high negative reward\n",
        "    # If the CDI is not close to 100%, the agent receives a reward inversely proportional to the distance from 100%\n",
        "    if self.sale_quantity == 0:\n",
        "      done = True\n",
        "      if 95 <= self.current_cdi <= 110:\n",
        "        self.combinations[self.current_episode] = deepcopy(self.all_quantities_matched[self.current_episode])\n",
        "        reward += 100000\n",
        "      elif self.current_cdi < 0:\n",
        "        reward += -10000\n",
        "      else:\n",
        "        reward += 1000 / abs(self.current_cdi - 100)\n",
        "\n",
        "    elif self.current_step >= self.max_steps:\n",
        "      reward -= 1000\n",
        "      done = True\n",
        "\n",
        "    if done:\n",
        "      self.render()\n",
        "      self.cdis[self.current_episode] = self.current_cdi\n",
        "      self.data = self.utils.update_dataframes(self.current_episode, self.original_data, self.data, self.all_quantities_matched[self.current_episode])\n",
        "      self.all_quantities_matched = [np.zeros(x['purchase'].shape[0]) for x in self.original_data]\n",
        "      self.data[self.current_episode] = deepcopy(self.original_data[self.current_episode])\n",
        "\n",
        "    self.current_step += 1\n",
        "    return self.state, reward, done, {}\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Resets the environment to the initial state, wich means going to the next sale (chosen randomly)\n",
        "\n",
        "    Returns:\n",
        "        state (list): The initial state of the environment\n",
        "    \"\"\"\n",
        "    self.none_actions = 0\n",
        "    self.current_episode = np.random.randint(0, len(self.data))\n",
        "    self.current_step = 0\n",
        "    self.sale = self.data[self.current_episode][\"sale\"]\n",
        "    self.purchases = self.data[self.current_episode][\"purchase\"]\n",
        "    self.all_quantities = [np.zeros(x['purchase'].shape[0]) for x in self.original_data]\n",
        "    return self.get_state()\n",
        "\n",
        "  def get_state(self):\n",
        "    \"\"\" Returns the observation data for the current state. It is updated on each step of the environment and episode\n",
        "        It returns the sale sale quantity, purchase quantity, the ideal price and average purchase price delta, and ideal price\n",
        "\n",
        "        Returns:\n",
        "            state (list): The observation data for the current state\n",
        "    \"\"\"\n",
        "    self.sale_quantity = self.sale[\"Quantidade\"]\n",
        "    self.di = self.sale['DI']\n",
        "    self.du = self.sale['du']\n",
        "\n",
        "    purchase_index = self.current_step % self.purchases.shape[0]\n",
        "    self.purchase_quantity = self.purchases.iloc[purchase_index][\"Quantidade\"]\n",
        "\n",
        "    self.max_steps = self.purchases.shape[0] * 5\n",
        "\n",
        "    self.ideal_price = self.utils.calculate_ideal_price(self.sale[\"Preço\"], self.du, self.di)\n",
        "\n",
        "    self.purchase_average_price = self.utils.calculate_average_price(self.all_quantities_matched[self.current_episode], self.purchases)\n",
        "\n",
        "    self.current_cdi = self.utils.calculate_cdi(self.sale[\"Preço\"], self.purchase_average_price, self.du, self.di)\n",
        "    purchase_quantity_matched = self.all_quantities_matched[self.current_episode][purchase_index]\n",
        "    self.state = [self.sale_quantity, self.purchase_quantity, abs(self.ideal_price - self.purchase_average_price), purchase_quantity_matched]\n",
        "    return self.state\n",
        "\n",
        "  def render(self):\n",
        "    \"\"\"\n",
        "    Prints the current CDI, sale initial quantity, remaining sale quantity, and purchases matched quantity for each episode\n",
        "    \"\"\"\n",
        "    headers = [\"Description\", \"Value\"]\n",
        "    data = [\n",
        "        (\"Current CDI\", self.current_cdi),\n",
        "        (\"Sale Initial Quantity\", self.original_data[self.current_episode]['sale'][\"Quantidade\"]),\n",
        "        (\"Remaining Sale Quantity\", self.data[self.current_episode]['sale'][\"Quantidade\"]),\n",
        "        (\"Purchases Matched Quantity\", sum(self.all_quantities_matched[self.current_episode]))\n",
        "    ]\n",
        "\n",
        "    print(tabulate(data, headers=headers, tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87641de1",
      "metadata": {
        "id": "87641de1"
      },
      "source": [
        "# Agente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nn2ojZe2ynYa",
      "metadata": {
        "id": "Nn2ojZe2ynYa"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "  \"\"\" Implements the DQN agent to learn how to combine purchases and sales to maximize the CDI around 100%\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               train_env,\n",
        "               test_env,\n",
        "               gamma=0.9,\n",
        "               epsilon=1.0,\n",
        "               epsilon_decay=0.994,\n",
        "               epsilon_min=0.1,\n",
        "               buffer_size=2000,\n",
        "               lr=1e-6,\n",
        "               batch_size=32):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        train_env (object): The environment for training the agent\n",
        "        test_env (object): The environment for testing the agent\n",
        "        gamma (float): The discount factor\n",
        "        epsilon (float): The exploration rate\n",
        "        epsilon_decay (float): The rate at which the exploration rate decays\n",
        "        epsilon_min (float): The minimum exploration rate\n",
        "        buffer_size (int): The size of the replay buffer\n",
        "        lr (float): The learning rate for the agent\n",
        "        batch_size (int): The size of the batch used for training the agent\n",
        "    \"\"\"\n",
        "    self.train_env = train_env\n",
        "    self.test_env = test_env\n",
        "    self.input_dim = self.train_env.observation_space.shape[0]\n",
        "    self.output_dim = self.train_env.action_space.n\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.epsilon_min = epsilon_min\n",
        "    self.buffer_size = buffer_size\n",
        "    self.lr = lr\n",
        "    self.batch_size = batch_size\n",
        "    self.model = self.build_model()\n",
        "\n",
        "  def train(self, episodes):\n",
        "    \"\"\"\n",
        "    Trains the agent for a given number of episodes using the DQN algorithm\n",
        "\n",
        "    Args:\n",
        "        episodes (int): The number of episodes to train the agent\n",
        "\n",
        "    Returns:\n",
        "        cdis (list): The CDIs for each episode\n",
        "    \"\"\"\n",
        "    memory = deque(maxlen=self.buffer_size)\n",
        "    local_epsilon = self.epsilon\n",
        "    for e in range(episodes):\n",
        "        episode_reward = 0\n",
        "        state = self.train_env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "          # Epsilon-greedy policy (exploration and exploitation)\n",
        "          if np.random.rand() < local_epsilon:\n",
        "              action = self.train_env.action_space.sample()\n",
        "          else:\n",
        "            q_values = self.model.predict(np.array([next_state]), verbose=0)[0]\n",
        "            action = np.argmax(q_values)\n",
        "          next_state, reward, done, _ = self.train_env.step(action)\n",
        "          memory.append((state, action, reward, next_state, done))\n",
        "          state = next_state\n",
        "          episode_reward += reward\n",
        "\n",
        "        # Update the model using the replay buffer\n",
        "        if len(memory) >= self.batch_size:\n",
        "          batch_sample = random.sample(memory, self.batch_size)\n",
        "\n",
        "          for state, action, reward, next_state, done in batch_sample:\n",
        "              target = reward\n",
        "              # Q-learning update rule (Bellman equation)\n",
        "              # Q(s, a) = r + γ * max(Q(s', a'))\n",
        "              if not done:\n",
        "                  target += self.gamma * np.amax(self.model.predict(np.array([next_state]), verbose=0)[0])\n",
        "              target_f = self.model.predict(np.array([next_state]), verbose=0)\n",
        "              target_f[0][action] = target\n",
        "              self.model.fit(np.array([state]), target_f, epochs=1, verbose=0)\n",
        "\n",
        "        print(f\"Episode: {e + 1}, Reward: {episode_reward}\")\n",
        "        local_epsilon = max(self.epsilon_min, self.epsilon_decay*local_epsilon)\n",
        "    return self.train_env.cdis\n",
        "\n",
        "  def evaluate(self, num_runs):\n",
        "    \"\"\"\n",
        "    Evaluates the agent for a given number of runs using the trained model\n",
        "\n",
        "    Args:\n",
        "        num_runs (int): The number of runs to evaluate the agent\n",
        "\n",
        "    Returns:\n",
        "        cdis (list): The CDIs for each run\n",
        "    \"\"\"\n",
        "    for s in tqdm(range(num_runs), desc=\"Evaluating\", unit=\"run\"):\n",
        "      episode_reward = 0\n",
        "      state = self.test_env.reset()\n",
        "      done = False\n",
        "      while not done:\n",
        "        q_values = self.model.predict(np.array([state]), verbose=0)[0]\n",
        "        action = np.argmax(q_values)\n",
        "        next_state, reward, done, _ = self.test_env.step(action)\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "      print(f\"Episode: {s + 1}, Reward: {episode_reward}\")\n",
        "    return self.test_env.cdis\n",
        "\n",
        "  def build_model(self):\n",
        "    \"\"\"\n",
        "    Builds the DQN model using a neural network with 3 hidden layers and ReLU activation functions\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(24, input_dim=self.input_dim, activation='relu'))\n",
        "    model.add(keras.layers.Dense(24, activation='relu'))\n",
        "    model.add(keras.layers.Dense(24, activation='relu'))\n",
        "    model.add(keras.layers.Dense(self.output_dim, activation='linear'))\n",
        "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=self.lr), loss='mse')\n",
        "    return model\n",
        "\n",
        "  def save_model(self, name):\n",
        "    \"\"\"\n",
        "    Saves the trained model to a file\n",
        "    \"\"\"\n",
        "    self.model.save(name)\n",
        "\n",
        "  def load_model(self, name):\n",
        "    \"\"\"\n",
        "    Loads a trained model from a file\n",
        "    \"\"\"\n",
        "    self.model = tf.keras.models.load_model(name)\n",
        "\n",
        "  def plot_cdis(self, cdis):\n",
        "    \"\"\"\n",
        "    Plots the distribution of CDIs\n",
        "    \"\"\"\n",
        "\n",
        "    # Get only the non-zero CDIs\n",
        "    calculated_cdis = []\n",
        "    for i in range(len(cdis)):\n",
        "      if cdis[i] != 0:\n",
        "        calculated_cdis.append(cdis[i])\n",
        "\n",
        "    # Plot the distribution of CDIs\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(calculated_cdis, bins=50, edgecolor='k', alpha=0.7)\n",
        "    plt.axvline(x=80, color='r', linestyle='--', linewidth=2, label='DI = 80')\n",
        "    plt.axvline(x=100, color='g', linestyle='--', linewidth=2, label='DI = 100')\n",
        "    plt.axvline(x=120, color='b', linestyle='--', linewidth=2, label='DI = 120')\n",
        "    plt.title('Distribution of CDIs')\n",
        "    plt.xlabel('CDI Values')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PK4RgQco1-gm",
      "metadata": {
        "id": "PK4RgQco1-gm"
      },
      "outputs": [],
      "source": [
        "train_env = Matching(dataframes_train)\n",
        "test_env = Matching(dataframes_test)\n",
        "agent = Agent(train_env, test_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QddBm7H32waK",
      "metadata": {
        "id": "QddBm7H32waK"
      },
      "outputs": [],
      "source": [
        "episodes = 1000\n",
        "cdis = agent.train(episodes)\n",
        "agent.plot_cdis(cdis)\n",
        "agent.save_model(\"../data/model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EaD1SzZC3JDk",
      "metadata": {
        "id": "EaD1SzZC3JDk"
      },
      "outputs": [],
      "source": [
        "agent.load_model(\"../data/model.h5\")\n",
        "runs = 500\n",
        "cdis = agent.evaluate(runs)\n",
        "agent.plot_cdis(cdis)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "conda_tensorflow2_p310",
      "language": "python",
      "name": "conda_tensorflow2_p310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
